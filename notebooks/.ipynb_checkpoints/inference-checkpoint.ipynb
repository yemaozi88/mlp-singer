{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21c63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "#import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "repos_mlps_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(repos_mlps_dir)\n",
    "import default_settings as default\n",
    "from data.preprocess import Preprocessor\n",
    "from utils import load_config, load_model, make_directory, set_seed\n",
    "\n",
    "sys.path.append(default.repos_dir)\n",
    "from vocoders import hifigan as hfg\n",
    "\n",
    "from sak import display as dp\n",
    "from vocoders import default_settings as default_vcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28deaac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference():\n",
    "    def __init__(\n",
    "        self,\n",
    "        preprocessor_config_path,\n",
    "        checkpoint_path\n",
    "        ):\n",
    "        self.preprocessor_config = load_config(preprocessor_config_path)\n",
    "        self.preprocessor = Preprocessor(self.preprocessor_config)\n",
    "        self.model = load_model(checkpoint_path, eval=True)\n",
    "\n",
    "    \n",
    "    def load_notes(self, mid_path, txt_path):\n",
    "        notes, phonemes = self.preprocessor.prepare_inference(\n",
    "            mid_path, txt_path\n",
    "        )\n",
    "        return notes, phonemes\n",
    "    \n",
    "    \n",
    "    def inference(self, notes, phonemes, mel_path=None):\n",
    "        chunk_size = self.model.seq_len\n",
    "        preds = []\n",
    "        total_len = len(notes)\n",
    "        notes = notes.to(device)\n",
    "        phonemes = phonemes.to(device)\n",
    "        remainder = total_len % chunk_size\n",
    "        mel_dim = self.preprocessor_config.n_mel_channels\n",
    "        \n",
    "        ## inference.\n",
    "        #@torch.no_grad()\n",
    "        if remainder:\n",
    "            pad_size = chunk_size - remainder\n",
    "            padding = torch.zeros(pad_size, dtype=int).to(device)\n",
    "            phonemes = torch.cat((phonemes, padding))\n",
    "            notes = torch.cat((notes, padding))\n",
    "            batch_phonemes = phonemes.reshape(-1, chunk_size)\n",
    "            batch_notes = notes.reshape(-1, chunk_size)\n",
    "            preds = self.model(batch_notes, batch_phonemes)\n",
    "            preds = preds.reshape(-1, mel_dim)[:-pad_size]\n",
    "        else:\n",
    "            batch_phonemes = phonemes.reshape(-1, chunk_size)\n",
    "            batch_notes = notes.reshape(-1, chunk_size)\n",
    "            preds = self.model(batch_notes, batch_phonemes)\n",
    "            mel_dim = preds.size(-1)\n",
    "            preds = preds.reshape(-1, mel_dim)\n",
    "        preds = preds.transpose(0, 1).unsqueeze(0)\n",
    "        \n",
    "        if not mel_path==None:\n",
    "            np.save(mel_path, preds.detach().numpy())\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af08c9f",
   "metadata": {},
   "source": [
    "## Initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8207adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## experimental settings.\n",
    "device = \"cpu\" # 'cpu' or 'cuda'\n",
    "\n",
    "# MLP_singer.\n",
    "svs_dir  = r'/home/akikun/projects/svs'\n",
    "work_dir = os.path.join(svs_dir, 'csd_hifigan_universal')\n",
    "\n",
    "checkpoint_path = os.path.join(work_dir, 'checkpoints', 'trained', 'best.pt')\n",
    "preprocessor_config_path = os.path.join(work_dir, 'configs', 'preprocess.json')\n",
    "\n",
    "# hifi_gan.\n",
    "checkpoint_hifigan_path = r'/home/akikun/common/pre_trained_vocoders/hifigan/g_02500000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e683bb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading '/home/akikun/common/pre_trained_vocoders/hifigan/g_02500000'\n",
      "Complete.\n",
      "Removing weight norm...\n"
     ]
    }
   ],
   "source": [
    "## svs\n",
    "infer = Inference(\n",
    "    preprocessor_config_path, \n",
    "    checkpoint_path\n",
    ")\n",
    "\n",
    "## hifigan.\n",
    "global h\n",
    "h = hfg.load_config(default_vcd.hifigan_config_path)\n",
    "global device_vocode\n",
    "generator, device_vocode = hfg.load_model(default_vcd.hifigan_model_path, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0de4a5",
   "metadata": {},
   "source": [
    "## Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788a8331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svs(mid_path, txt_path, mel_path, pred_path):\n",
    "    ## load notes and phonemes. \n",
    "    notes, phonemes = infer.load_notes(mid_path, txt_path)\n",
    "\n",
    "    ## inference.\n",
    "    preds = infer.inference(notes, phonemes, mel_path=mel_path)\n",
    "\n",
    "    ## vocode.\n",
    "    x = preds.cpu().detach().numpy()\n",
    "    mel = torch.FloatTensor(x).to(device_vocode)\n",
    "    hfg.vocode(mel, generator, h, pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe7d3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## directories.\n",
    "\n",
    "# data directory in where the directory 'wav', 'mid' and 'txt' are included.\n",
    "data_dir = os.path.join(svs_dir, 'data')\n",
    "\n",
    "# the directory where the predicted wav will be saved.\n",
    "pred_dir = os.path.join(work_dir, 'preds')\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)\n",
    "\n",
    "# the directory where the predicted mel will be saved.\n",
    "mel_dir = os.path.join(work_dir, 'mel')\n",
    "if not os.path.exists(mel_dir):\n",
    "    os.makedirs(mel_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0abe51a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking arugment for argument index in method wrapper_index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3540668/1162546818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpred_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{song}.wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3540668/2122461248.py\u001b[0m in \u001b[0;36msvs\u001b[0;34m(mid_path, txt_path, mel_path, pred_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m## inference.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphonemes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m## vocode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3540668/4059391431.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, notes, phonemes, mel_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mbatch_phonemes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphonemes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mbatch_notes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_notes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_phonemes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpad_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/svs/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/mlp-singer/model/core.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pitch, phonemes)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphonemes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mpitch_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpitch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpitch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphonemes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpitch_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/svs/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/svs/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/miniconda3/envs/svs/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking arugment for argument index in method wrapper_index_select)"
     ]
    }
   ],
   "source": [
    "# wav id to infer.\n",
    "song = 'kr001a'\n",
    "\n",
    "mid_path = os.path.join(data_dir, \"mid\", f\"{song}.mid\")\n",
    "txt_path = os.path.join(data_dir, \"txt\", f\"{song}.txt\")\n",
    "\n",
    "mel_path  = os.path.join(mel_dir, f\"{song}.npy\")\n",
    "pred_path = os.path.join(pred_dir, f\"{song}.wav\")\n",
    "\n",
    "svs(mid_path, txt_path, mel_path, pred_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6455ba",
   "metadata": {},
   "source": [
    "## listen to the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1fda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred0_dir = r'/home/akikun/projects/svs/csd_hifigan_universal/box/wav'\n",
    "pred0_path = os.path.join(pred0_dir, f\"{song}_generated_e2e.wav\")\n",
    "dp.disp_wav(pred_path)\n",
    "dp.disp_wav(pred0_path)\n",
    "\n",
    "mel = np.load(mel_path)\n",
    "mel = mel.reshape(np.shape(mel)[1:])\n",
    "dp.disp_spectrogram(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fcac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sak import signal_processing as sp\n",
    "# p = sp.load_wav(pred_path)\n",
    "# r = sp.load_wav(raw_path)\n",
    "# print(np.shape(p))\n",
    "# print(np.shape(r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
